---
title: "2-data_cleaning_and_descriptive"
author: "Erva"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# set directory
```{r}
setwd("your_directory")
file_path="your_directory"
```

# input- output
```{r}
input="output_1_after_quality_check.csv"
output= "output_2_after_quality_check.csv"
```

# libraries
```{r}
library(esc)
library(dplyr) 
library(purrr)  # For map functions
library(ggplot2)
library(gridExtra)
library(tidyverse) # needed for 'glimpse'
library(meta)
library(metafor)
library(dmetar)
library(robumeta)
library(clubSandwich)
library(rmarkdown)
library(metaviz)
library(gghalves)  #for geom_half_boxplot
library(tidyr)
library(readr)
library(sf)
library(viridis)
library(knitr)
library(kableExtra)
library(modelsummary)
```

# plot theme
```{r}
library(ggplot2)

# Define a color-blind friendly palette
color_blind_friendly_palette <- c(
  "#B1B424",  # Olive Yellow (new, to bridge between yellow and green)
  "#4C92C3",  # Azure (new, slightly darker than sky blue)
  "#8673A1",  # Lavender Indigo (new, to bridge between blue and purple)
  "#8FBC8F",  # Dark Sea Green
  "#009E73",  # Bluish Green
  "#56B4E9",  # Sky Blue
  "#0072B2",  # Blue
  "#6C8EBF",  # Soft Blue
  "#5E81AC",  # Queen Blue (new, a darker soft blue)
  "#483D8B",  # Dark Slate Blue
  "#CC79A7",  # Reddish Purple
  "#CC6677",  # Soft Red
  "#DECBE4",  # Pale Purple
  "#D1B2A5",  # Sand
  "#A89984",  # Taupe (new, to bridge between sand and grey)
  "#999933",  # Olive Green
  "#BDB76B",  # Dark Khaki
  "#8B4513",  # Saddle Brown
  "#949494",  # Grey
  "#777777",  # Medium Grey (new, to smooth transition to darker greys)
  "#525252",  # Dark Grey
  "#893F45"
)

# Define the plot theme 
plot_theme <- theme_minimal(base_size = 14) +  # Slightly larger base font size for better readability
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "black"),  # Black for clarity and emphasis
    plot.subtitle = element_text(face = "plain", size = 14, color = "grey70"),  # Subtitle in a lighter grey for hierarchy
    legend.title = element_text(face = "bold", color = "black"),  # Black for consistency with titles
    legend.text = element_text(size = 12),
    axis.title = element_text(face = "bold", size = 14, color = "black"),  # Black for main axis titles
    axis.text = element_text(color = "black"),  # Black for axis text for maximum readability
    panel.grid.major.x = element_blank(),  # Remove major grid lines along x
    panel.grid.major.y = element_blank(),  # Remove major grid lines along y
    panel.grid.minor.x = element_blank(),  # Remove minor grid lines along x
    panel.grid.minor.y = element_blank(),  # Remove minor grid lines along y
    axis.line = element_line(color = "black"),  # Add black line for x and y axes
    legend.position = "bottom",  # Legend at the bottom to not overlap with data visualization
    legend.background = element_rect(fill = "white", color = NA),  # White background for legend for clarity
    legend.box.background = element_rect(color = "grey80"),  # Light grey border for legend
    legend.box.margin = margin(5, 5, 5, 5)
  )

# Apply the theme to a sample plot
sample_plot <- ggplot(mpg, aes(x = displ, y = hwy, color = class)) +
  geom_point(alpha = 0.6) +
  scale_color_manual(values = color_blind_friendly_palette) +
  labs(
    title = "Engine Displacement vs. Highway Miles Per Gallon",
    subtitle = "Data from the 'mpg' dataset",
    x = "Engine Displacement (L)",
    y = "Miles Per Gallon (Highway)",
    color = "Vehicle Class"
  ) +
  plot_theme

# Print the plot
#print(sample_plot)


```

#0-  Parameters
```{r}
# correlation between within-study effects
rho=0.8
threshold_for_moderator=2
```

```{r}
adjust_stars <- function(sig) {
  # Reduces each level by one star
  # Assumes sig can be "*", "**", "***"; modifies to "", "*", "**"
  ifelse(sig == "***", "**", 
         ifelse(sig == "**", "*", 
                ifelse(sig == "*", "", sig)))
}
```


# 1- Data Cleaning & Preparation
```{r}
# 1- Read the data from the CSV file
ma_feedback_time_data <- read_csv(input)

# 2- Replace "no_info" and "NA " values with NA
ma_feedback_time_data[ma_feedback_time_data == "no_info"] <- NA
ma_feedback_time_data[ma_feedback_time_data == "NA"] <- NA

# first define experiment number
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(experiment_number = paste0("Experiment ", expt_num))

# 3- Convert necessary columns to numeric
numeric_columns <- c('year', 'expt_num', 'feedback_delay_difference', 'mean_age', 'sd_age', 'min_age', 'max_age', 'gender', 'n_excluded', 'retention_interwal.day._i', 'retention_interwal.day._d', 'nb_times_feedback_provided_i', 'nb_times_feedback_provided_d', 'nb_question.item_in_training', 
'N_total', 'n_i', 'n_d', 'mean_i', 'mean_d', 'sd_i', 'sd_d', 'se_i', 'se_d',
'lower_CI', 'upper_CI', 't', 'SEM', 'F', 'F_sign_change', 'p', 'eta_square',
'partial_eta_square', 'odd_ratio', 'log_odd', 'SE_log_odd', 'corr', 'power', 'beta',
'SE_beta', 'n_point', 'effect_size_original', 'SE_original', 'effect_size_by_calculator', 
'se_by_calculator', 'effect_size', 'standard_error', 'effect_size_alternative', 
'standard_error_alternative', 'n', 'hedges_g', 'se_g','retention_interval_difference_last_feedback_posttest_immediate_vs_delayed', 'feedback_delay_difference_converted_to_time.sec.')

# Convert columns to numeric, safely and checking for existence
ma_feedback_time_data[numeric_columns] <- lapply(numeric_columns, function(col) {
  if (col %in% names(ma_feedback_time_data)) {
    # Safely convert to numeric
    safe_conversion <- suppressWarnings(as.numeric(as.character(ma_feedback_time_data[[col]])))
    if (any(is.na(safe_conversion) & !is.na(ma_feedback_time_data[[col]]))) {
      warning(paste("NA introduced by coercion in column:", col))
    }
    return(safe_conversion)
  } else {
    warning(paste("Warning: Column", shQuote(col), "not found in the DataFrame."))
    return(ma_feedback_time_data[[col]]) # Return the original column if it exists
  }
})

# 4- Log transformations where applicable

ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(feedback_delay_difference_converted = feedback_delay_difference_converted_to_time.sec.)
         
         
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(feedback_delay_difference = if_else(delay_definition == "second", log(feedback_delay_difference), feedback_delay_difference),
         retention_interval_difference_last_feedback_posttest_immediate_vs_delayed = log(retention_interval_difference_last_feedback_posttest_immediate_vs_delayed+ 0.0001),
         feedback_delay_difference_converted_to_time.sec. = log(feedback_delay_difference_converted_to_time.sec.))

# 5- Recode values in a categorical column
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(experimental_vs_curriculum_based_task = recode(experimental_vs_curriculum_based_task,
                                                        "experimental_task" = "experimental",
                                                        "curriculum_based_tasl" = "curriculum_based"))
# 6- Create new columns based on conditions
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(
    retention_interval = ifelse(retention_interwal.day._i == retention_interwal.day._d, 
                                retention_interwal.day._i, 
                                NA),  # Use generic NA which adapts to numeric context
    
    feedback_provided = ifelse(feedback_provided_i == feedback_provided_d, 
                               feedback_provided_i, 
                               NA_character_),  # Explicitly use NA_character_ if these are character columns
    
    feedback_i_d_rate = ifelse(!is.na(nb_times_feedback_provided_i) & !is.na(nb_times_feedback_provided_d), 
                               nb_times_feedback_provided_i / nb_times_feedback_provided_d, 
                               NA),  # Use generic NA which adapts to numeric context
    
    feedback_remind_answer = ifelse(feedback_remind_answer_i == feedback_remind_answer_d, 
                                    feedback_remind_answer_i, 
                                    NA_character_)  # Explicitly use NA_character_ if these are character columns
  )

# 7- Define and apply classification functions
classify_feedback_type <- function(feedback) {
  if (startsWith(feedback, "mixed")) {
    if (grepl("verification", feedback)) {
      return("mixed_include_verification")
    } else {
      return("mixed_not_include_verification")
    }
  } else {
    return(feedback)
  }
}

classify_feedback_type_kr_kcr_ef_tryagain <- function(feedback) {
  if (grepl("mixed\\(verification\\+correct_response\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(verification\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(correct_response\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(verification\\+try_again\\)", feedback)) {
    return("try_again")
  } else if (grepl("mixed\\(verification\\+correct_response\\)", feedback)) {
    return("correct_response")
  } else {
    return(feedback)
  }
}


# Define the classify_feedback_type_simple_elaborated function
classify_feedback_type_simple_elaborated <- function(feedback) {
  if (grepl("mixed\\(verification\\+correct_response\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(verification\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(correct_response\\+elaborated\\)", feedback)) {
    return("elaborated")
  } else if (grepl("mixed\\(verification\\+correct_response\\)", feedback)) {
    return("simple")
  } else if (grepl("\\bverification\\b", feedback)) {
    return("simple")
  } else if (grepl("\\bcorrect_response\\b", feedback)) {
    return("simple")
  } else if (grepl("mixed\\(verification\\+try_again\\)", feedback)) {
    return("simple")
  } else {
    return(feedback)
  }
}


# Apply the function to create the new column
ma_feedback_time_data$feedback_type_mixed_grouped <- sapply(ma_feedback_time_data$feedback_type, classify_feedback_type)
ma_feedback_time_data$feedback_type_kr_kcr_ef_tryagain <- sapply(ma_feedback_time_data$feedback_type, classify_feedback_type_kr_kcr_ef_tryagain)
ma_feedback_time_data$feedback_type_simple_elaborated <- sapply(ma_feedback_time_data$feedback_type, classify_feedback_type_simple_elaborated)

# add variance
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(
    vi_g = se_g^2,  # Create 'vi' as the square of 'se_g'
    g_ID = as.character(paste("es", seq_len(nrow(ma_feedback_time_data)), sep = "_"))  # Ensure 'es_ID' is character
  )

ma_feedback_time_data <- ma_feedback_time_data %>%
  group_by(unique_ID, expt_num) %>%
  mutate(unique_sample_ID = cur_group_id()) %>%
  ungroup() 

# add new education_level (all others that are not "tertiary" categorized as "other")
# ma_feedback_time_data <- ma_feedback_time_data %>%
#   mutate(
#     educational_level_tertiary_others = if_else(educational_level == "tertiary", "tertiary", "other")
#   )

# ma_feedback_time_data <- ma_feedback_time_data %>%
#   mutate(
#     educational_level_tertiary_others = case_when(
#       educational_level %in% c("tertiary", "adult_education") ~ "tertiary+adult",
#       educational_level %in% c("primary", "secondary") ~ "primary+secondary",
#       TRUE ~ "other"  # This will categorize all remaining levels as "other"
#     )
#   )

# 8- Create and save the analysis data frame
necessary_columns <- c('unique_ID', 'g_ID', 'unique_sample_ID','expt_num', 'effect_size', 'standard_error', 'n', 'hedges_g', 'se_g', 'vi_g')
moderator_columns <- c('country_data_collected_from','publication_type','mean_age', 'gender','feedback_type', 'feedback_type_mixed_grouped', 'delay_definition','feedback_delay_difference','educational_level','learning_domain', 
                    'task_level', 'experimental_vs_curriculum_based_task','participant_design', 'arbitrary_learning','item_posttest', 
                    'delayed_feedback_given_in', 'retention_interval', 'feedback_provided', 'feedback_remind_answer', 'nb_question.item_in_training', 
                    'posttest_category', 'learning_material_type_in_training','memory_retrieval_vs_knowledge_application', 
                    'feedback_type_kr_kcr_ef_tryagain', 'feedback_type_simple_elaborated','year','time_limit_for_answer', 'time_limit_for_answer_include_not_reported',
                    'feedback_i_d_rate','interval_btw_last_feedback_and_posttest_controlled', 'prior_knowledge','feedback_delay_difference_converted_to_time.sec.', 'retention_interval_difference_last_feedback_posttest_immediate_vs_delayed','experiment_number','feedback_delay_difference_converted')


# 9- Recode values in a categorical column to eliminate typos etc
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(arbitrary_learning = recode(arbitrary_learning,
                                                        "yes" = "arbitrary",
                                                        "no" = "non_arbitrary"))

ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(feedback_remind_answer = recode(feedback_remind_answer,
                                                        "yes" = "remind",
                                                        "no" = "not_remind"))

ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(time_limit_for_answer = recode(time_limit_for_answer,
                                                        "yes" = "answer_time_limited",
                                                        "no" = "answer_time_not_limited"))


# time limit including not reported
ma_feedback_time_data <- ma_feedback_time_data %>%
  mutate(time_limit_for_answer_include_not_reported = if_else(
    is.na(time_limit_for_answer_not_indicated_explicitly) | time_limit_for_answer_not_indicated_explicitly != "yes",
    time_limit_for_answer,
    NA_character_  # Use the appropriate NA type
  ))

 ma_feedback_time_data <- ma_feedback_time_data %>%
   mutate(time_limit_for_answer_include_not_reported = replace_na(time_limit_for_answer_include_not_reported, "not_reported"))
 
 
ma_feedback_time_data_analysis <- ma_feedback_time_data %>%
  select(all_of(c(necessary_columns, moderator_columns))) %>%
  rename(se_effect_size = standard_error, sample_size = n, nb_item_in_training = nb_question.item_in_training, 
         retention_interval_difference_btw_lastfeedback_posttest = retention_interval_difference_last_feedback_posttest_immediate_vs_delayed,
         feedback_delay_difference_converted_to_time_log = feedback_delay_difference_converted_to_time.sec.)


## some cleaning re-coding for moderator analysis
ma_feedback_time_data_analysis <- ma_feedback_time_data_analysis %>%
  mutate(task_level = recode(task_level, "mixed" = "higher_order"))

ma_feedback_time_data_analysis <- ma_feedback_time_data_analysis %>%
  mutate(memory_retrieval_vs_knowledge_application = recode(memory_retrieval_vs_knowledge_application, "mixed" = "knowledge_application"))




# 11- Save to new CSV file
write_csv(ma_feedback_time_data_analysis, output)

# 12- Print the first 5 rows of the new DataFrame
print(head(ma_feedback_time_data_analysis))
```

# 2- Descriptive

## 2.1- Country- World Map

### Map Function

```{r}
# Mapping of country names from your dataset to the shapefile names
name_mapping <- c(
  "US" = "United States of America",
  "UK" = "United Kingdom",
  "Saudi_Arabia" = "Saudi Arabia"
)

# Applying the mapping to your data
ma_feedback_time_data_analysis <- ma_feedback_time_data_analysis %>%
  mutate(
    country_data_collected_from = ifelse(
      country_data_collected_from %in% names(name_mapping),
      name_mapping[country_data_collected_from],
      country_data_collected_from
    )
  )

# Function for world map plots
plot_map <- function(fill, data = ma_feedback_time_data_analysis){
  # Download "Admin 0 – Countries" from
  # https://www.naturalearthdata.com/downloads/110m-cultural-vectors/
  world_map <- read_sf("ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp") %>% 
    # remove Antarctica
    filter(ISO_A3 != "ATA")
  
  # make summary data to plot on map
  participants_by_country = data %>% 
    group_by(country_data_collected_from, unique_sample_ID) %>% 
    summarise(participants_per_sample = max(sample_size)) %>% 
    group_by(country_data_collected_from) %>% 
    summarize(participants = sum(participants_per_sample)) %>% 
    # uneven numbers may occur where we didn't have information on how many 
    # participants in treatment vs. control group and assumed an even split 
    # (i.e. dividing overal n by number of condition)
    mutate_if(is.numeric, round, digits = 0)
  
  map_data <- data %>% 
    group_by(country_data_collected_from) %>% 
    summarise(papers = n_distinct(unique_ID), 
              samples = n_distinct(unique_sample_ID),
              effects = n_distinct(g_ID), 
              yi = mean(hedges_g)
    ) %>% 
    left_join(participants_by_country) %>% 
    # make cutoff versions that allow for better plotting
    mutate(papers_cut = case_when(papers == 1 ~ "1", 
                                  papers <= 5 ~ "2-5", 
                                  papers <= 10 ~ "5-10", 
                                  papers > 10 ~ as.character(max(papers))), 
           papers_cut = fct_relevel(as.factor(papers_cut), "1", "2-5","5-10", as.character(max(papers))),
           samples_cut = case_when(samples == 1 ~ "1", 
                                   samples <= 5 ~ "2-5", 
                                   samples <= 10 ~ "5-10", 
                                   samples > 10 ~ as.character(max(samples))), 
           samples_cut = fct_relevel(as.factor(samples_cut), "1", "2-5","5-10", as.character(max(samples))),
           effects_cut = case_when(effects == 1 ~ "1", 
                                   effects <= 5 ~ "2-5", 
                                   effects <= 10 ~ "5-10", 
                                   effects <= 15 ~ "10-15",
                                   effects > 15 ~ ">15"), 
           effects_cut = fct_relevel(as.factor(effects_cut), "1", "2-5","5-10", "10-15",">15"),
           participants_cut = case_when(participants < 500 ~ "< 500", 
                                        participants < 1000 ~ "500 - 1'000",  
                                        participants < 2000 ~ "1'000 - 2'000",
                                        participants < 5000 ~ "2'000 - 5'000",
                                        participants < 10000 ~ "5'000 - 10'000",
                                        participants < 15000 ~ "10'000 - 15'000",
                                        participants > 15000 ~ as.character(max(participants))),
           participants_cut = fct_relevel(as.factor(participants_cut), "< 500", 
                                          "500 - 1'000","1'000 - 2'000", "2'000 - 5'000", 
                                          "5'000 - 10'000", "10'000 - 15'000", 
                                          as.character(max(participants))),
           participants_num = ifelse(participants <=5000, participants, 5001)
    )
  
  # combine geo data and summarized data
  map <- left_join(world_map, map_data, by = c("ADMIN" = "country_data_collected_from"))
  
  if (fill == "papers") {
    # papers
    papers <- ggplot() + 
      geom_sf(data = map, 
              aes(fill = papers_cut),
              size = 0.25) +
      coord_sf(crs = st_crs("ESRI:54030")) +  # Robinson
      scale_fill_viridis_d(option = "plasma", begin = 0.2, end = 0.9) +
      labs(fill = "Papers") +
      plot_theme +
      theme_void() +
      theme(legend.position = "bottom")
    return(papers)
  }
  
  if (fill == "samples") {
    # samples
    samples <- ggplot() + 
      geom_sf(data = map, 
              aes(fill = samples_cut),
              size = 0.25, 
              alpha = 0.6) +
      coord_sf(crs = st_crs("ESRI:54030")) +  # Robinson
      scale_fill_viridis_d(option = "plasma", begin = 0.8, end = 1, direction = -1) +
      labs(fill = "Samples") +
      plot_theme +
      theme_void() +
      theme(legend.position = "bottom") 
    return(samples)
  }
  
  
  if (fill == "participants") {
    
    # Define the number of colors in the scale
    n_colors <- 20000
    
    # Generate the color scale
    palette <- colorRampPalette(c("#FCFDBFCC", "black"), interpolate = "linear")(n_colors)
    
    # # alternative from viridis
    # palette <- viridis(n_colors, 
    #                    alpha = 0.8, begin = 0, end = 1, direction = -1, option = "rocket")
    
    # assign (more or less) proportional color
    colors <- c(palette[1], palette[1000], palette[2000], palette[3500], palette[7500], palette[12500], palette[length(palette)])
    
    
    participants <- ggplot() + 
      geom_sf(data = map , 
              aes(fill = participants_cut),
              size = 0.25) +
      coord_sf(crs = st_crs("ESRI:54030")) +  # Robinson
      scale_fill_manual(values = colors, 
                        na.value = "white") +
      labs(fill = "Participants") +
      theme_void() +
      theme(legend.position = "bottom")
    return(participants)
  }
  
  if(fill == "effects") {
    
    # Define the number of colors in the scale
    n_colors <- 15
    
    # generate color scale
    palette <- viridis(n_colors,
                       alpha = 0.8, begin = 0.65, end = 1, direction = -1, option = "rocket")
    
    # # Manual Alternative 
    # # Define the start and end points for the color scale
    # start_color <- "#FFFFE5"  # Very light yellow
    # end_color <- "#FFD700"    # Dark yellow, almost orange
    # 
    # # Generate the color scale
    # palette <- colorRampPalette(c(start_color, end_color), interpolate = "linear")(n_colors)
    
    # assign (more or less) proportional color
    colors <- c(palette[1], palette[5], palette[12], palette[15], "black")
    
    
    effects <- ggplot() + 
      geom_sf(data = map , 
              aes(fill = effects_cut),
              size = 0.25) +
      coord_sf(crs = st_crs("ESRI:54030")) +  # Robinson
      scale_fill_manual(values = colors, 
                        na.value = "white", 
                        guide = guide_legend(nrow = 1)) +
      labs(fill = "Effect sizes") +
      theme_void() +
      theme(legend.position = "bottom",
            legend.text = element_text(size = 10),  # Adjust the text size
            legend.key.height = unit(0.5, "lines"),  # Set the height of the legend keys
            legend.box.spacing = unit(0.1, "lines")  # Adjust the spacing between the legend keys
            )
    return(effects)
  }
}

```

Run map funciton
!! need ne_110m_admin_0_countries to be downloaded from: https://www.naturalearthdata.com/downloads/110m-cultural-vectors/

```{r}
effects_map_plot <- plot_map(fill = "papers", data = ma_feedback_time_data_analysis)

effects_map_plot

# Save the plot to a file
ggsave("effects_map_plot.png", plot = effects_map_plot, width = 10, height = 8, dpi = 300)

```


## 2.2- Distributions

```{r}

plot_distribution <- function(data, variable, variable_type, grouping_value) {
  
  # Define a named vector for mapping
  grouping_columns <- c(
      papers = "unique_ID",
      effects = "g_ID",
      samples = "unique_sample_ID"
  )
  
  # Retrieve the appropriate column name using the grouping_value parameter
  if (!grouping_value %in% names(grouping_columns)) {
      stop("Invalid grouping_value. Use 'papers', 'effects', or 'samples'.")
  }
  grouping_column <- grouping_columns[[grouping_value]]
  
  # Prepare the data: summarize it based on the variable type
  data_summary <- data %>%
    group_by_at(vars(one_of(grouping_column))) %>%
    summarise(
      value = if(variable_type == "continuous") {
        max(get(variable), na.rm = TRUE)
      } else {
        if(n_distinct(get(variable)) == 1) {
          first(get(variable))  # If only one unique value, use it
        } else {
          warning(paste("Multiple unique values found in", deparse(substitute(data)), "for group", cur_group_id(), "- cannot summarize uniquely."))
          NA_character_  # Use NA to indicate non-unique categorical data
        }
      },
      .groups = 'drop'
    )

  # plot
 
  # Count NA values
  #na_count <- sum(is.na(data[[variable]]))
 na_counts <- data %>%
    group_by_at(vars(one_of(grouping_column))) %>%
    summarise(na_count = sum(is.na(get(variable))), .groups = 'drop')
 na_count <-nrow(subset(na_counts, na_count != 0))
 
 
 
  # Further processing for categorical data to count occurrences of each value
  if(variable_type == "categorical") {
    value_counts <- data_summary %>%
      group_by(value) %>%
      summarise(count = n(), .groups = 'drop')
    
    # Determine the number of unique categories
    num_categories <- n_distinct(value_counts$value)
    
  # Choose the palette based on the number of categories
  palette <- if(num_categories > 20) {
    viridis(num_categories, option = "D")  # using viridis for many categories
  } else {
    color_blind_friendly_palette  # predefined palette for fewer categories
  }

    # Generate bar plot for categorical data
    plot <- ggplot(value_counts, aes(x = value, y = count, fill = value)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = count), vjust = -0.3, size = 3.5) +
      scale_fill_manual(values = palette) +
      labs(x = gsub("_", " ", variable), y = "Count", title = "") +
      #labs(x = gsub("_", " ", variable), y = "Count", title = paste("Distribution of", variable, "grouped by", grouping_value)) +
      scale_x_discrete(labels = function(x) gsub("_", " ", x)) +
      coord_flip() +
      plot_theme +
      theme(legend.position = "none")
  } else if(variable_type == "continuous") {
    
    
    # Compute statistics
        stats <- data_summary %>%
      summarise(
        mean = mean(as.numeric(value[!is.infinite(value) & !is.na(value)])),
        sd = sd(as.numeric(value[!is.infinite(value) & !is.na(value)])),
        se = sd / sqrt(n()),
        lower_ci = mean - qt(0.975, df = n() - 1) * se,
        upper_ci = mean + qt(0.975, df = n() - 1) * se,
        max = max(as.numeric(value[!is.infinite(value) & !is.na(value)])), min = min(as.numeric(value[!is.infinite(value) & !is.na(value)])),
        .groups = 'drop',
      )

    # Continuous data: Use a scatter plot with a density line and error bars for confidence intervals
        
    plot <- ggplot(data_summary, aes(x = value)) +
      geom_histogram(bins = 100, fill = "#0072B2", color = "black", alpha = 0.5) +
      geom_vline(xintercept = stats$mean, color = "#CC79A7", linetype = "dashed", size = 1) +
      geom_point(data = stats, aes(x = mean, y = 0), color = "#CC79A7", size = 4) +
      #geom_errorbar(data = stats, aes(x = mean, ymin = lower_ci, ymax = upper_ci), width = 0.2, color = "#CC79A7") +
      geom_density(aes(y = ..scaled..), fill = NA, color = "#0072B2", alpha = 0.7) +
      geom_label(x = Inf, y = Inf, label = paste("Mean:", round(stats$mean, 2) ,"\nSD:", round(stats$sd, 2), "\nMax:", round(stats$max,2), "\nMin:", round(stats$min,2), "\nNb NA:", na_count), hjust = 1.1, vjust = 1.1, size = 3, fill = "white") +
      labs(x = gsub("_", " ", variable), y = "Density/Frequency", title = paste("Distribution of", variable, "grouped by", grouping_value, "\nNA values:", na_count)) +
      plot_theme
  } else {
    stop("Invalid variable_type. Use 'categorical' or 'continuous'.")
  }

  return(plot)
}

```

### Descriptives

```{r}

# country
plot_descriptive_country_data_collected_from=plot_distribution(data = ma_feedback_time_data_analysis, variable = "country_data_collected_from", variable_type = "categorical", grouping_value = "effects")
plot_descriptive_country_data_collected_from
ggsave("plot_descriptive_country_data_collected_from.png", plot = plot_descriptive_country_data_collected_from, width = 8, height = 6, dpi = 300)

plot_descriptive_country_data_collected_from=plot_distribution(data = ma_feedback_time_data_analysis, variable = "country_data_collected_from", variable_type = "categorical", grouping_value = "papers")
plot_descriptive_country_data_collected_from
ggsave("plot_descriptive_country_data_collected_from.png", plot = plot_descriptive_country_data_collected_from, width = 8, height = 6, dpi = 300)

# nb effect size per paper
plot_descriptive_nb_effect_size=plot_distribution(data = ma_feedback_time_data_analysis, variable = "unique_ID", variable_type = "categorical", grouping_value = "effects")
plot_descriptive_nb_effect_size
ggsave("plot_descriptive_nb_effect_size.png", plot = plot_descriptive_nb_effect_size, width = 8, height = 6, dpi = 300)

analysis_results <- ma_feedback_time_data_analysis %>%
  group_by(unique_ID) %>%
  summarise(n_effect_per_paper = n(), .groups = 'drop')

# Step 2: Calculate mean, minimum, mode, and maximum of the counts
summary_stats <- analysis_results %>%
  summarise(
    Mean = mean(n_effect_per_paper),
    Min = min(n_effect_per_paper),
    Max = max(n_effect_per_paper),
    Mode = as.numeric(names(which.max(table(n_effect_per_paper))))  # Mode calculation
  )

# Print the summary statistics
print(summary_stats)

# cumulative sample size:
grouped_sample_size <- ma_feedback_time_data_analysis %>%
  group_by(unique_ID) %>%
  summarise(total_sample_size = sum(sample_size), .groups = 'drop')

cumulative_sample_size <- sum(grouped_sample_size$total_sample_size)
cat("Cumulative sample size from all papers is", cumulative_sample_size)


# sample size distribution per papers
plot_descriptive_sample_size=plot_distribution(data = ma_feedback_time_data_analysis, variable = "sample_size", variable_type = "continuous", grouping_value = "papers")
plot_descriptive_sample_size
ggsave("plot_descriptive_sample_size.png", plot = plot_descriptive_sample_size, width = 8, height = 6, dpi = 300)

# gender distribution per papers
plot_descriptive_gender=plot_distribution(data = ma_feedback_time_data_analysis, variable = "gender", variable_type = "continuous", grouping_value = "papers")
plot_descriptive_gender
ggsave("plot_descriptive_gender.png", plot = plot_descriptive_gender, width = 8, height = 6, dpi = 300)

# mean age distribution per papers
plot_descriptive_mean_age=plot_distribution(data = ma_feedback_time_data_analysis, variable = "mean_age", variable_type = "continuous", grouping_value = "papers")
plot_descriptive_mean_age
ggsave("plot_descriptive_mean_age.png", plot = plot_descriptive_mean_age, width = 8, height = 6, dpi = 300)


# publication year distribution per papers
plot_descriptive_year=plot_distribution(data = ma_feedback_time_data_analysis, variable = "year", variable_type = "continuous", grouping_value = "papers")
plot_descriptive_year
ggsave("plot_descriptive_year.png", plot = plot_descriptive_year, width = 8, height = 6, dpi = 300)


# publication type distribution per papers
plot_descriptive_publication_type=plot_distribution(data = ma_feedback_time_data_analysis, variable = "publication_type", variable_type = "categorical", grouping_value = "papers")
plot_descriptive_publication_type
ggsave("plot_descriptive_publication_type.png", plot = plot_descriptive_publication_type, width = 8, height = 6, dpi = 300)


# publication type distribution per papers
plot_descriptive_publication_type=plot_distribution(data = ma_feedback_time_data_analysis, variable = "publication_type", variable_type = "categorical", grouping_value = "effects")
plot_descriptive_publication_type
ggsave("plot_descriptive_publication_type.png", plot = plot_descriptive_publication_type, width = 8, height = 6, dpi = 300)


#plot_distribution(data = ma_feedback_time_data_analysis, variable = "learning_domain", variable_type = "categorical", grouping_value = "effects")

# hedges g distribution
plot_descriptive_nb_effect_size=plot_distribution(data = ma_feedback_time_data_analysis, variable = "hedges_g", variable_type = "continuous", grouping_value = "papers")
plot_descriptive_nb_effect_size
ggsave("plot_descriptive_nb_effect_size.png", plot = plot_descriptive_nb_effect_size, width = 8, height = 6, dpi = 300)
```


# 3- Pooling the Effect Sizes 
```{r}
g_feedback_time_corr  <-  robu(formula = hedges_g ~ 1, data = ma_feedback_time_data_analysis,
                            studynum = unique_ID, var.eff.size = vi_g,
                            rho = rho,  modelweights = "CORR", small = FALSE)
print(g_feedback_time_corr)

# sensitivity analysis for different rho
sensitivity(g_feedback_time_corr)


# Extract for Latex
# Extract reg_table from your model
reg_table <- g_feedback_time_corr$reg_table

# Number of effect sizes (k)
k <- nrow(g_feedback_time_corr$data.full)

# Main description for the subset
subset_description <- "Immediate vs. delayed feedback"

# Assuming reg_table is derived from your 'robu' model
# We'll place each value in its corresponding column now
results_table <- tibble(
  Statistic = c(
    "Immediate vs. delayed feedback", 
    "Num. Obs.", "N", "rho", "I^2", "tau^2"
  ),
  Estimate = c(
    as.character(round(g_feedback_time_corr$reg_table$b.r, 2)),  # g value
    as.character(nrow(g_feedback_time_corr$data.full)),  # Num. Obs.
    as.character(length(unique(g_feedback_time_corr$data.full$study))),  # N
    as.character(g_feedback_time_corr$mod_info$rho),  # rho
    as.character(format(g_feedback_time_corr$mod_info$I.2, digits = 2)),  # I^2
    as.character(format(g_feedback_time_corr$mod_info$tau.sq, digits = 4))  # tau^2
  ),
  `Std. Error` = c(
    as.character(round(g_feedback_time_corr$reg_table$SE, 2)),  # SE value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `t Statistic` = c(
    as.character(round(g_feedback_time_corr$reg_table$t, 2)),  # t value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `P value` = c(
    format.pval(g_feedback_time_corr$reg_table$prob, digits = 3, eps = 0.001, na.form = "<.001"),  # P-value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `95% CI` = c(
    paste0("[", round(g_feedback_time_corr$reg_table$CI.L, 2), ", ", round(g_feedback_time_corr$reg_table$CI.U, 2), "]"),  # 95% CI
    "", "", "", "", ""  # Empty for additional stats
  ),
  `Sig` = c(
    g_feedback_time_corr$reg_table$sig,  # Significance
    "", "", "", "", ""  # Empty for additional stats
  )
)

# Create the LaTeX table using kableExtra
latex_table <- results_table %>%
  kbl(
    format = "latex",
    booktabs = TRUE,
    align = "lllllll",  # Alignment for each column
    col.names = c("Statistic", "Estimate", "Std. Error", "t Statistic", "P value", "95% CI", "Sig")
  ) %>%
  kable_styling(
    latex_options = c("hold_position"),
    full_width = FALSE
  ) %>%
  add_header_above(c(" " = 1, "Robust Variance Estimates Model" = 6)) %>%
  row_spec(
    0,  # Insert a horizontal line after the description of the subset
    extra_latex_after = "\\midrule"
  )

# Write the LaTeX table to a .tex file
tex_filename <- normalizePath(file.path(file_path, "model_with_outlier.tex"), mustWork = FALSE)
tryCatch({
  cat(latex_table, file = tex_filename)
  if (file.exists(tex_filename)) {
    message("File successfully created: ", tex_filename)
  } else {
    warning("Failed to create file: ", tex_filename)
  }
}, error = function(e) {
  warning("An error occurred while writing file: ", e$message)
})

# Print the LaTeX table
cat(latex_table)

```

# 4- Outliers 

From preregistration
 
We will check the adjusted effect sizes (Hedges' g) for potential outliers by assessing whether any effect sizes deviate by more than three standard deviations from the group mean effect sizes. This step ensures that no single study disproportionately influences the summary statistics or introduces bias into our analysis. This method is adapted from Van der Kleij et al. (2015) in their study on feedback effects in computer-based learning environments.

## 4.a - Outliers Detection
```{r}
# Calculate the mean and standard deviation of Hedges' g
mean_hedges_g <- mean(ma_feedback_time_data_analysis$hedges_g, na.rm = TRUE)
sd_hedges_g <- sd(ma_feedback_time_data_analysis$hedges_g, na.rm = TRUE)

# Define the cutoff for outliers (more than 3 standard deviations from the mean)
lower_bound <- mean_hedges_g - 3 * sd_hedges_g
upper_bound <- mean_hedges_g + 3 * sd_hedges_g

# Identify outliers
ma_feedback_time_data_analysis$outlier_prereg <- with(ma_feedback_time_data_analysis, hedges_g < lower_bound | hedges_g > upper_bound)

# Output the studies flagged as outliers
outliers <- ma_feedback_time_data_analysis[ma_feedback_time_data_analysis$outlier_prereg,]
print(outliers)

# Create the histogram
outliers_check_histogram_plot.png <- ggplot(ma_feedback_time_data_analysis, aes(x = hedges_g, fill = as.factor(outlier_prereg))) +
  geom_histogram(binwidth = 0.1, color = "black", alpha = 0.7) +
  geom_vline(xintercept = mean_hedges_g, linetype = "dashed", color = "black", size = 1, label = "Mean") +
  geom_vline(xintercept = lower_bound, linetype = "dotted", color = "#D55E00", size = 1, label = "-3 SD") +
  geom_vline(xintercept = upper_bound, linetype = "dotted", color = "#D55E00", size = 1, label = "+3 SD") +
  scale_fill_manual(values = c("FALSE" = "#0072B2", "TRUE" = "#CC79A7"),  # Using color-blind friendly colors
                    name = "", 
                    labels = c("FALSE" = "Non-Outlier", "TRUE" = "Outlier")) +
  labs(title = "",
       subtitle = paste0("Mean: ", round(mean_hedges_g, 3), ", SD: ", round(sd_hedges_g, 3)),
       x = "Hedges' g", 
       y = "Frequency") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "black"),
    plot.subtitle = element_text(face = "plain", size = 14, color = "grey70"),
    legend.title = element_text(face = "bold", color = "black"),
    legend.text = element_text(size = 12),
    axis.title = element_text(face = "bold", size = 14, color = "black"),
    axis.text = element_text(color = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    legend.background = element_rect(fill = "white", color = NA),
    legend.box.background = element_rect(color = "grey80"),
    legend.box.margin = margin(5, 5, 5, 5)
  )

# Print the histogram plot
print(outliers_check_histogram_plot.png)

# Save the plot as an image
ggsave("outliers_check_histogram_plot.png", plot = outliers_check_histogram_plot.png, width = 10, height = 8, dpi = 300)

# Remove outliers from the data for further analysis
data_clean <- ma_feedback_time_data_analysis[!ma_feedback_time_data_analysis$outlier_prereg,]

```

## 4.b- Re-pooling effect sizes without outliers

```{r}
g_feedback_time_corr_without_outliers  <-  robu(formula = hedges_g ~ 1, data = data_clean,
                            studynum = unique_ID, var.eff.size = vi_g,
                            rho = rho,  modelweights = "CORR", small = FALSE)
g_feedback_time_corr_without_outliers

# sensitivity analysis for different rho
sensitivity_table=sensitivity(g_feedback_time_corr_without_outliers)

# Generate the LaTeX table using kable and kableExtra
latex_table_sensitivity <- kable(
  sensitivity_table, 
  format = "latex", 
  booktabs = TRUE, 
  align = "lcccccc"
) %>%
  kable_styling(latex_options = c("hold_position")) %>% # No 'striped' option
  add_header_above(c(" " = 1, "Sensitivity Analysis RVE Model" = 6)) 

# Write the LaTeX table to a .tex file
tex_filename <- normalizePath(file.path(file_path, "sensitivity_rve.tex"), mustWork = FALSE)
tryCatch({
  cat(latex_table_sensitivity, file = tex_filename)
  if (file.exists(tex_filename)) {
    message("File successfully created: ", tex_filename)
  } else {
    warning("Failed to create file: ", tex_filename)
  }
}, error = function(e) {
  warning("An error occurred while writing file: ", e$message)
})


# Print the LaTeX table to console
cat(latex_table_sensitivity)



# Extract for Latex
# Extract reg_table from your model
reg_table <- g_feedback_time_corr_without_outliers$reg_table

# Number of effect sizes (k)
k <- nrow(g_feedback_time_corr_without_outliers$data.full)

# Main description for the subset
subset_description <- "Immediate vs. delayed feedback"

# Assuming reg_table is derived from your 'robu' model
# We'll place each value in its corresponding column now
results_table <- tibble(
  Statistic = c(
    "Immediate vs. delayed feedback", 
    "Num. Obs.", "N", "rho", "I^2", "tau^2"
  ),
  Estimate = c(
    as.character(round(g_feedback_time_corr_without_outliers$reg_table$b.r, 2)),  # g value
    as.character(nrow(g_feedback_time_corr_without_outliers$data.full)),  # Num. Obs.
    as.character(length(unique(g_feedback_time_corr_without_outliers$data.full$study))),  # N
    as.character(g_feedback_time_corr_without_outliers$mod_info$rho),  # rho
    as.character(format(g_feedback_time_corr_without_outliers$mod_info$I.2, digits = 2)),  # I^2
    as.character(format(g_feedback_time_corr_without_outliers$mod_info$tau.sq, digits = 4))  # tau^2
  ),
  `Std. Error` = c(
    as.character(round(g_feedback_time_corr_without_outliers$reg_table$SE, 2)),  # SE value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `t Statistic` = c(
    as.character(round(g_feedback_time_corr_without_outliers$reg_table$t, 2)),  # t value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `P value` = c(
    format.pval(g_feedback_time_corr_without_outliers$reg_table$prob, digits = 3, eps = 0.001, na.form = "<.001"),  # P-value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `95% CI` = c(
    paste0("[", round(g_feedback_time_corr_without_outliers$reg_table$CI.L, 2), ", ", round(g_feedback_time_corr_without_outliers$reg_table$CI.U, 2), "]"),  # 95% CI
    "", "", "", "", ""  # Empty for additional stats
  ),
  `Sig` = c(
    g_feedback_time_corr_without_outliers$reg_table$sig,  # Significance
    "", "", "", "", ""  # Empty for additional stats
  )
)

# Create the LaTeX table using kableExtra
latex_table <- results_table %>%
  kbl(
    format = "latex",
    booktabs = TRUE,
    align = "lllllll",  # Alignment for each column
    col.names = c("Statistic", "Estimate", "Std. Error", "t Statistic", "P value", "95% CI", "Sig")
  ) %>%
  kable_styling(
    latex_options = c("hold_position"),
    full_width = FALSE
  ) %>%
  add_header_above(c(" " = 1, "Robust Variance Estimates Model" = 6)) %>%
  row_spec(
    0,  # Insert a horizontal line after the description of the subset
    extra_latex_after = "\\midrule"
  )

# Write the LaTeX table to a .tex file
tex_filename <- normalizePath(file.path(file_path, "model_without_outlier.tex"), mustWork = FALSE)
tryCatch({
  cat(latex_table, file = tex_filename)
  if (file.exists(tex_filename)) {
    message("File successfully created: ", tex_filename)
  } else {
    warning("Failed to create file: ", tex_filename)
  }
}, error = function(e) {
  warning("An error occurred while writing file: ", e$message)
})

# Print the LaTeX table
cat(latex_table)
```

# 5- Forest Plot

```{r}
# Load necessary libraries
library(htmltools)   # For saving HTML files
library(webshot)     # For saving plots as images (optional, but recommended)
library(knitr)       # For including images in HTML
library(magrittr)    # For using the %>% pipe operator

# Save the forest plot as a PNG file
png(filename = "forest_plot.png", width = 8.5, height = 56, units = "in", res = 300) # APA7 paper size
forest.robu(
  g_feedback_time_corr_without_outliers, 
  es.lab = "experiment_number", 
  study.lab = "unique_ID", 
  "Weight" = r.weights, 
  "Effect Size" = hedges_g
)
dev.off()  # Close the image file

# Create an HTML file and embed the image
html_file <- htmltools::tagList(
  tags$html(
    tags$head(tags$title("")),
    tags$body(
      h1(""),
      tags$img(src = "forest_plot.png", width = "100%") # Embed the PNG image
    )
  )
)

# Save the HTML file
htmltools::save_html(html_file, file = "forest_plot.html")

```
## interactive forest plot

```{r}
library(ggplot2)
library(plotly)

# Assuming data_clean is already prepared with the necessary columns
# Create ggplot object for the forest plot
forest_plot <- ggplot(data_clean, aes(x = hedges_g, y = reorder(unique_ID, hedges_g))) +
  geom_point() +
  geom_errorbarh(aes(xmin = hedges_g - 1.96 * sqrt(vi_g), xmax = hedges_g + 1.96 * sqrt(vi_g)), height = 0.1)

# Convert the ggplot object to a plotly object and specify dimensions
interactive_forest_plot <- ggplotly(forest_plot) %>%
  layout(
    title = "",
    xaxis = list(title = "Effect Size (Hedges' g)"),
    yaxis = list(title = "Study ID"),
    autosize = FALSE,
    width = 1000,   # Width in pixels
    height = 1200,    # Height in pixels
       plot_bgcolor = 'rgba(0,0,0,0)',  # Transparent plot background
    paper_bgcolor = 'rgba(0,0,0,0)'  # Transparent paper background
  )

# Render the plot
interactive_forest_plot

```


# 6- Publication Bias

ss## 6.1- funnel plot before trimfill
```{r}
result_model= g_feedback_time_corr_without_outliers$reg_table
## define estimate and se
estimate = result_model$b.r
se = result_model$SE

#Store a vector of values that spans the range from 0 to the max value of impression (standard error) in your dataset.
#Make the increment (the final value) small enough (I choose 0.001) to ensure your whole range of data is captured
se.seq=seq(0, max(data_clean$se_g), 0.001)

#Compute vectors of the lower-limit and upper limit values for
#the 95% CI region
ll95 = estimate-(1.96*se.seq)
ul95 = estimate+(1.96*se.seq)

#Do this for a 99% CI region too
ll99 = estimate-(3.29*se.seq)
ul99 = estimate+(3.29*se.seq)

#And finally, calculate the confidence interval for your meta-analytic estimate 
# directly capture from the RVE model
meanll95 = result_model$CI.L
meanul95 = result_model$CI.U

#Put all calculated values into one data frame
#You might get a warning about '...row names were found from a short variable...' 
#You can ignore it.
dfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)

#Draw Plot
library(ggplot2)
fp_fixD = ggplot(aes(x = se_g, y = hedges_g), data = data_clean) +
  geom_point(shape = 1) +
  xlab('Standard Error') + ylab('Effect sizes (Hedges g)') + 
  geom_line(aes(x = se.seq, y = ll95), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul95), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ll99), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul99), linetype = 'dashed', data = dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='solid', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='solid', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = estimate, xend = max(se.seq), yend = estimate), linetype='solid', colour ="red", data=dfCI) +
  scale_x_reverse() +
  scale_y_continuous(breaks=seq(-12.5,2.50,0.50))+
  coord_flip()+
  theme_classic() +
  plot_theme

fp_fixD 

ggsave("funnel_before_correction.png", plot = fp_fixD, width = 10, height = 8, dpi = 300)

```

## 6.2- Eggers Test

```{r}
# Prepare the data frame required by eggers test
data_for_eggers_test <- data.frame(
  effect_size = g_feedback_time_corr_without_outliers$data.full$effect.size,
  std_error = sqrt(g_feedback_time_corr_without_outliers$data.full$var.eff.size),
  variance=g_feedback_time_corr_without_outliers$data.full$var.eff.size
)

eggers_test <- data_for_eggers_test %>% 
  mutate(y = effect_size/sqrt(variance), x = 1/sqrt(variance)) %>% 
  lm(y ~ x, data = .) 

eggers_test

# Constructing the unified result table
result_egger <- tibble(
  Statistic = c(
    "Intercept", "Inverse SE", 
    "Num. Obs.", "R²", "Adjusted R²", "AIC", "BIC", "Log Likelihood", "RMSE"
  ),
  Estimate = round(c(
    coef(summary(eggers_test))[1, 1], coef(summary(eggers_test))[2, 1], 
    nobs(eggers_test), 
    summary(eggers_test)$r.squared, summary(eggers_test)$adj.r.squared,
    AIC(eggers_test), BIC(eggers_test), as.numeric(logLik(eggers_test)), sigma(eggers_test)
  ), 2),
  `Std. Error` = c(
    round(coef(summary(eggers_test))[1, 2], 2), round(coef(summary(eggers_test))[2, 2], 2), 
    "", "","","","","",""
  ),
  `t Statistic` = c(
    sprintf("t = %.2f", coef(summary(eggers_test))[1, 3]), sprintf("t = %.2f", coef(summary(eggers_test))[2, 3]),
    "", "","","","","",""
  ),
  `P value` = c(
    ifelse(coef(summary(eggers_test))[1, 4] < 0.001, "p < 0.001", sprintf("p = %.3f", coef(summary(eggers_test))[1, 4])),
    ifelse(coef(summary(eggers_test))[2, 4] < 0.001, "p < 0.001", sprintf("p = %.3f", coef(summary(eggers_test))[2, 4])),
    "", "","","","","",""
  )
)

# Create the LaTeX table using kableExtra with a horizontal line after the "Inverse SE"
latex_table <- result_egger %>%
  kbl(
    format = "latex",
    booktabs = TRUE,
    align = "lllll",  # Alignment for each column
    col.names = c("Statistic", "Estimate", "Std. Error", "t Statistic", "P value")
  ) %>%
  kable_styling(
    latex_options = c("hold_position"),
    full_width = FALSE
  ) %>%
  add_header_above(c(" " = 1, "Egger's Regression Test" = 4)) %>%
  row_spec(
    2, # The row right after 'Inverse SE', which is the third row (index is 0-based in row_spec)
    extra_latex_after = "\\midrule"
  ) %>%
  footnote(
    general = "Results of Egger's regression test. No adjustments have been made.",
    threeparttable = TRUE
  )

   # Write the LaTeX table to a .tex file
    tex_filename <- normalizePath(file.path(file_path, "table_eggers_test.tex"), mustWork = FALSE)
    
    tryCatch({
      cat(latex_table, file = tex_filename)
      if (file.exists(tex_filename)) {
        message("File successfully created: ", tex_filename)
      } else {
        warning("Failed to create file: ", tex_filename)
      }
    }, error = function(e) {
      warning("An error occurred while writing file for moderator '", moderator, "': ", e$message)
    })
  

# Print the LaTeX table
cat(latex_table)

```


## 6.3- Funnel Plot after trimfill 

```{r}
# 1- correction
# first we need to run a rma model using metafor package since trimfill cannot be applied to multidimentional models
rma_model_for_trimfill <- rma(hedges_g, vi_g, data=data_clean, measure="OR")

tf_result <- trimfill(rma_model_for_trimfill)
tf_result

# 2-Find the filled data
# Create the filled_data data frame
filled_data <- data.frame(
  EffectSize = tf_result$yi,
  Variance = tf_result$vi,
  StandartError = sqrt(tf_result$vi) 
)

# Add unique_ID column
filled_data$unique_ID <- NA  # Initialize all as NA to handle different lengths
filled_data$new_old<- NA
# Fill in original IDs
original_length <- min(length(data_clean$unique_ID), length(filled_data$EffectSize))
filled_data$unique_ID[1:original_length] <- data_clean$unique_ID
filled_data$new_old[1:original_length] <- 0
# Generate new unique IDs for extra imputed rows, if any
if (length(filled_data$EffectSize) > length(data_clean$unique_ID)) {
  # Starting point for new IDs
  new_id_start <- length(data_clean$unique_ID) + 1
  # Number of new IDs needed
  new_ids_count <- length(filled_data$EffectSize) - length(data_clean$unique_ID)
  # Create new IDs
  new_ids <- seq(from = new_id_start, length.out = new_ids_count)
  # Assign new IDs to the additional rows
  filled_data$unique_ID[(original_length + 1):length(filled_data$EffectSize)] <- new_ids
  filled_data$new_old[(original_length + 1):length(filled_data$EffectSize)] <- 1
}

head(filled_data)
tail(filled_data)  # To check the newly assigned IDs at the end

# 3- Rerun the model
corrected_g_feedback_time_corr_without_outliers  <-  robu(formula = EffectSize ~ 1, data = filled_data,
                            studynum = unique_ID, var.eff.size = Variance,
                            rho = 0.8,  modelweights = "CORR", small = FALSE)
print(corrected_g_feedback_time_corr_without_outliers)

# Extract for Latex
# Extract reg_table from your model
reg_table <- corrected_g_feedback_time_corr_without_outliers$reg_table

# Number of effect sizes (k)
k <- nrow(corrected_g_feedback_time_corr_without_outliers$data.full)

# Main description for the subset
subset_description <- "Immediate vs. delayed feedback"

# Assuming reg_table is derived from your 'robu' model
# We'll place each value in its corresponding column now
results_table <- tibble(
  Statistic = c(
    "Immediate vs. delayed feedback", 
    "Num. Obs.", "N", "rho", "I^2", "tau^2"
  ),
  Estimate = c(
    as.character(round(corrected_g_feedback_time_corr_without_outliers$reg_table$b.r, 2)),  # g value
    as.character(nrow(corrected_g_feedback_time_corr_without_outliers$data.full)),  # Num. Obs.
    as.character(length(unique(corrected_g_feedback_time_corr_without_outliers$data.full$study))),  # N
    as.character(corrected_g_feedback_time_corr_without_outliers$mod_info$rho),  # rho
    as.character(format(corrected_g_feedback_time_corr_without_outliers$mod_info$I.2, digits = 2)),  # I^2
    as.character(format(corrected_g_feedback_time_corr_without_outliers$mod_info$tau.sq, digits = 4))  # tau^2
  ),
  `Std. Error` = c(
    as.character(round(corrected_g_feedback_time_corr_without_outliers$reg_table$SE, 2)),  # SE value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `t Statistic` = c(
    as.character(round(corrected_g_feedback_time_corr_without_outliers$reg_table$t, 2)),  # t value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `P value` = c(
    format.pval(corrected_g_feedback_time_corr_without_outliers$reg_table$prob, digits = 3, eps = 0.001, na.form = "<.001"),  # P-value
    "", "", "", "", ""  # Empty for additional stats
  ),
  `95% CI` = c(
    paste0("[", round(corrected_g_feedback_time_corr_without_outliers$reg_table$CI.L, 2), ", ", round(corrected_g_feedback_time_corr_without_outliers$reg_table$CI.U, 2), "]"),  # 95% CI
    "", "", "", "", ""  # Empty for additional stats
  ),
  `Sig` = c(
    corrected_g_feedback_time_corr_without_outliers$reg_table$sig,  # Significance
    "", "", "", "", ""  # Empty for additional stats
  )
)

# Create the LaTeX table using kableExtra
latex_table <- results_table %>%
  kbl(
    format = "latex",
    booktabs = TRUE,
    align = "lllllll",  # Alignment for each column
    col.names = c("Statistic", "Estimate", "Std. Error", "t Statistic", "P value", "95% CI", "Sig")
  ) %>%
  kable_styling(
    latex_options = c("hold_position"),
    full_width = FALSE
  ) %>%
  add_header_above(c(" " = 1, "Robust Variance Estimates Model (Corrected)" = 6)) %>%
  row_spec(
    0,  # Insert a horizontal line after the description of the subset
    extra_latex_after = "\\midrule"
  )

# Write the LaTeX table to a .tex file
tex_filename <- normalizePath(file.path(file_path, "model_trimfillcorrected.tex"), mustWork = FALSE)
tryCatch({
  cat(latex_table, file = tex_filename)
  if (file.exists(tex_filename)) {
    message("File successfully created: ", tex_filename)
  } else {
    warning("Failed to create file: ", tex_filename)
  }
}, error = function(e) {
  warning("An error occurred while writing file: ", e$message)
})

# Print the LaTeX table
cat(latex_table)



# 4- Funnel Plot with filled points

result_model= corrected_g_feedback_time_corr_without_outliers$reg_table
## define estimate and se
estimate = result_model$b.r
se = result_model$SE

#Store a vector of values that spans the range from 0 to the max value of impression (standard error) in your dataset.
#Make the increment (the final value) small enough (I choose 0.001) to ensure your whole range of data is captured
se.seq=seq(0, max(data_clean$se_g), 0.001)

#Compute vectors of the lower-limit and upper limit values for
#the 95% CI region
ll95 = estimate-(1.96*se.seq)
ul95 = estimate+(1.96*se.seq)

#Do this for a 99% CI region too
ll99 = estimate-(3.29*se.seq)
ul99 = estimate+(3.29*se.seq)

#And finally, calculate the confidence interval for your meta-analytic estimate 
# directly capture from the RVE model
meanll95 = result_model$CI.L
meanul95 = result_model$CI.U

#Put all calculated values into one data frame
#You might get a warning about '...row names were found from a short variable...' 
#You can ignore it.
dfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)

#Draw Plot
library(ggplot2)
fp_fixD = ggplot(aes(x = StandartError, y = EffectSize), data = filled_data) +
  geom_point(aes(fill = factor(new_old)),shape = 21) +
  xlab('Standard Error') + ylab('Effect sizes (Hedges g)') + 
  geom_line(aes(x = se.seq, y = ll95), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul95), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ll99), linetype = 'dashed', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul99), linetype = 'dashed', data = dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='solid', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='solid', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = estimate, xend = max(se.seq), yend = estimate), linetype='solid', colour ="red", data=dfCI) +
  scale_x_reverse() +
  scale_y_continuous(breaks=seq(-12.5,2.50,0.50))+
  coord_flip()+
  scale_fill_manual(values = c("0" = "white", "1" = "black")) +
  guides(fill = "none") + 
  theme_classic() +
   plot_theme

fp_fixD 

ggsave("funnel_after_correction.png", plot = fp_fixD, width = 10, height = 8, dpi = 300)

```

# 7- Moderators

## 7.1- Moderator Plots

### 7.1.a- Function: moderator plots
```{r}
moderator_plots <- function(data_input, name, threshold_for_moderator = threshold_for_moderator, moderators_list = NULL, common_plot = TRUE, remove_small_groups = TRUE) {
  
  # Use the provided moderators list or default list
  if (is.null(moderators_list)) {
    moderators <- c(
      "feedback_type_simple_elaborated", 
      "delay_definition", 
      "feedback_delay_difference_converted_to_time_log",
      "educational_level_tertiary_others",  
      "learning_domain", 
      "task_level", 
      "prior_knowledge", 
      "experimental_vs_curriculum_based_task"
    )
  } else {
    moderators <- moderators_list
  }
  
  # Empty list to store plots
  plot_list <- list()
  
  for (moderator in moderators) {
    
    data=data_input
    
    plot <- NULL
    
    # Calculate if the current moderator is continuous
    is_continuous <- is.numeric(data[[moderator]]) && !is.factor(data[[moderator]])
    message("Processing moderator: ", moderator, " | is_continuous: ", is_continuous)
    
    adjusted_color_palette <- color_blind_friendly_palette
    
    # Count total NAs for the current moderator before filtering small groups
    total_na_count <- sum(is.na(data[[moderator]]))
    
    # Check if the moderator is continuous
    #is_continuous <- is.numeric(data[[moderator]]) || n_distinct(data[[moderator]]) > 15
    
    if (!is_continuous && remove_small_groups) {
      # Count occurrences of each unique value in the moderator
      small_groups <- data %>%
        count(.data[[moderator]]) %>%
        filter(n < threshold_for_moderator) %>%
        pull(.data[[moderator]])  # Extract the names of small categories

      # Record original levels of the moderator before filtering
      original_levels <- levels(factor(data[[moderator]]))

      # Replace small groups with NA
      data <- data %>%
        mutate(across(all_of(moderator), ~ ifelse(. %in% small_groups, NA, .)))
      
      # Get remaining levels of the moderator after filtering
      remaining_levels <- levels(factor(data[[moderator]]))
      
      # Get indices of the removed categories
      removed_indices <- which(!(original_levels %in% remaining_levels))
      
      if (length(removed_indices) > 0) {
  adjusted_color_palette <- color_blind_friendly_palette[-removed_indices]
}
      
    }
    
    # Drop NA values for the moderator
    plot_data <- data %>%
      mutate(across(all_of(moderator), ~ factor(.))) %>%  
      drop_na(.data[[moderator]])
    
    # Replace underscores with spaces in the moderator's levels
plot_data <- plot_data %>%
  mutate(across(all_of(moderator), ~ gsub("_", " ", .)))
    
    # Skip plotting if no data left
    if (nrow(plot_data) == 0) {
      next
    }
    
    if (is_continuous) {
      # Ensure moderator is numeric for continuous plots
      plot_data[[moderator]] <- as.numeric(plot_data[[moderator]])
      
      # Check if variance of hedges_g is 0, skip if no variance
      if (var(plot_data$hedges_g, na.rm = TRUE) == 0) {
        next
      }
      
      min_y <- if (is.numeric(plot_data[[moderator]])) min(plot_data[[moderator]], na.rm = TRUE) else NA

          
      # Calculate total number of points for annotation
      n_points <- nrow(plot_data)
      
      # Plot for continuous moderator
      plot <- ggplot(plot_data, aes(x = hedges_g, y = .data[[moderator]])) +
        geom_point(aes(color = se_g), alpha = 0.6) +
        tryCatch({
          geom_smooth(method = "lm", se = TRUE, color = "darkred")
        }, error = function(e) {
          message("Error in geom_smooth for moderator: ", moderator)
          return(NULL)
        }) +
        # Add annotation with total number of points and total NAs
        annotate("text", x = max(plot_data$hedges_g, na.rm = TRUE), 
                 y = min_y, 
                 label = paste0("n = ", n_points, 
                                if (total_na_count > 0) paste0("\nNA = ", total_na_count) else ""), 
                 hjust = 1, vjust = -0.5, 
                 color = "#525252", 
                 fontface = "bold", 
                 size = 4) +
         labs(x = paste0("Hedge's g (", name, ")"), y = gsub("_", " ", moderator)) +
                geom_segment(aes(x = 0, y = min_y - 0.5, 
                         xend = max(plot_data$hedges_g, na.rm = TRUE), yend = min_y - 0.5), 
                     arrow = arrow(length = unit(0.2, "cm")), color = "#893F45") +
        geom_segment(aes(x = 0, y = min_y - 0.5, 
                         xend = min(plot_data$hedges_g, na.rm = TRUE), yend = min_y - 0.5), 
                     arrow = arrow(length = unit(0.2, "cm")), color = "#893F45") +
        annotate("text", x = max(plot_data$hedges_g, na.rm = TRUE) / 2, 
                 y = min_y - 1, 
                 label = "Immediate Feedback", hjust = 0.5, color = "#893F45") +
        annotate("text", x = min(plot_data$hedges_g, na.rm = TRUE) / 2, 
                 y = min(plot_data[[moderator]], na.rm = TRUE) - 1, 
                 label = "Delayed Feedback", hjust = 0.5, color = "#893F45")+  
        plot_theme + 
        theme(
          axis.title = element_text(size = rel(0.8), face = "plain"),
          panel.grid.major.x = element_line(color = "#F0F0F0", size = 0.5), 
          panel.grid.minor.x = element_line(color = "#F0F0F0", size = 0.25)
        )
      
    } else {
      # Calculate number of points in each category
      n_points_per_group <- plot_data %>%
        count(.data[[moderator]]) %>%
        rename(n = n)
      
       # Plot for categorical moderator
  plot <- ggplot(plot_data, aes(x = .data[[moderator]], y = hedges_g)) +
    geom_half_boxplot(aes(color = .data[[moderator]], group = .data[[moderator]]), 
                      side = "l", size = 0.5, nudge = 0.05, 
                      outlier.shape = NA, alpha = 0.7) +
    geom_half_violin(aes(fill = .data[[moderator]], group = .data[[moderator]]), 
                     side = "r", nudge = 0.05, 
                     alpha = 0.7) +
    geom_half_point(aes(color = .data[[moderator]], group = .data[[moderator]]), 
                    side = "l", 
                    position = position_jitter(width = 0.1, height = 0),
                    alpha = 0.7) +
    geom_hline(yintercept = 0, 
               linewidth = 0.5, linetype = "24", color = "grey") +
    # Horizontal arrows from center
    geom_segment(aes(x = 0.55, y = 0, xend = 0.55, yend = -1.5), 
                 arrow = arrow(length = unit(0.2, "cm")), color = "#893F45") +
    geom_segment(aes(x = 0.55, y = 0, xend = 0.55, yend = 1.5), 
                 arrow = arrow(length = unit(0.2, "cm")), color = "#893F45") +
    # Labels for arrows, repositioned for clarity
    annotate("text", x = 0.5, y = -0.5, label = "Delayed Feedback", hjust = 0.5, color = "#893F45") +
    annotate("text", x = 0.5, y = 0.5, label = "Immediate Feedback", hjust = 0.5, color = "#893F45") +
    # Annotate the number of points for each group
    geom_text(data = n_points_per_group, 
              aes(x = .data[[moderator]], 
                  y = max(plot_data$hedges_g, na.rm = TRUE) + 0.3, 
                  label = paste0("n = ", n)), 
              vjust = -0.5, 
              color = "#525252", 
              fontface = "bold", 
              size = 4) +
    scale_color_manual(values = adjusted_color_palette) +
    scale_fill_manual(values = adjusted_color_palette) +
    guides(color = "none", fill = "none") +
    labs(x = NULL, y = paste0("Hedge's g (", name, ")")) +
    plot_theme + 
    theme(
      axis.title = element_text(size = rel(0.8), face = "plain"),
      panel.grid.major.x = element_line(color = "#F0F0F0", size = 0.5), 
      panel.grid.minor.x = element_line(color = "#F0F0F0", size = 0.25)
    ) +
    coord_flip()
    }
    
    plot_list[[moderator]] <- plot
    
    
  }
  
    if (common_plot == TRUE) {
      plot <- ggpubr::ggarrange(
        plotlist = plot_list,
        labels = gsub("_", " ", moderators),
        font.label = list(size = 10), 
        ncol = 2, 
        nrow = 4
      ) %>%
      ggpubr::annotate_figure(
        top = ggpubr::text_grob(name, face = "bold", size = 12)
      )
      
      ggsave(filename = paste0(name, "_combined_plot.png"), plot = plot, width = 12, height = 10)
      return(plot)
    } else {
      names(plot_list) <- moderators
      for (i in seq_along(plot_list)) {
        ggsave(filename = paste0(name, "_", moderators[i], ".png"), plot = plot_list[[i]], width = 8, height = 6)
      }
      return(plot_list) 
    }
  
  
}

```


### 7.1.b- moderator plots for all moderators
```{r}
  # Define moderators
  moderators <- c('feedback_type', 'feedback_type_mixed_grouped', 'delay_definition','feedback_delay_difference','educational_level','learning_domain', 
                    'task_level', 'experimental_vs_curriculum_based_task','participant_design', 'arbitrary_learning','item_posttest', 
                    'delayed_feedback_given_in', 'retention_interval', 'feedback_provided', 'feedback_remind_answer', 'nb_item_in_training', 
                    'posttest_category', 'learning_material_type_in_training','memory_retrieval_vs_knowledge_application', 
                    'feedback_type_kr_kcr_ef_tryagain', 'feedback_type_simple_elaborated','year','time_limit_for_answer', 'time_limit_for_answer_include_not_reported',
                    'feedback_i_d_rate','interval_btw_last_feedback_and_posttest_controlled', 'prior_knowledge','feedback_delay_difference_converted_to_time_log', 'retention_interval_difference_btw_lastfeedback_posttest'
   
  )


# select the columns and rename 
for_moderator_plots <- data_clean %>%
  select(
    hedges_g,
    se_g,
    feedback_type_simple_elaborated, 
    delay_definition, 
    feedback_delay_difference_converted_to_time_log,
    educational_level,  
    learning_domain, 
    task_level, 
    prior_knowledge, 
    experimental_vs_curriculum_based_task
  ) %>%
  rename(
    feedback_type = feedback_type_simple_elaborated,
    delay_difference = feedback_delay_difference_converted_to_time_log,
    experimental_vs_curriculum_task = experimental_vs_curriculum_based_task
  )
   
preregistered_moderators_plot_list= c(
  "feedback_type", 
  "delay_definition", 
  "delay_difference",
  "educational_level",  
  "learning_domain", 
  "task_level", 
  "prior_knowledge", 
  "experimental_vs_curriculum_task"
)

for_moderator_plots_exploratory <- data_clean %>%
  select(hedges_g,
    se_g,
    participant_design,
    arbitrary_learning,
    item_posttest, 
    feedback_provided, 
    feedback_remind_answer, 
    nb_item_in_training, 
    memory_retrieval_vs_knowledge_application,
    year, 
    time_limit_for_answer_include_not_reported,
    retention_interval_difference_btw_lastfeedback_posttest) %>%
  rename(
    learning_type = arbitrary_learning,
    posttest_item = item_posttest,
    nb_training_item = nb_item_in_training,
    posttest_task=memory_retrieval_vs_knowledge_application,
    publication_year=year,
    answer_time=time_limit_for_answer_include_not_reported,
    retention_interval=retention_interval_difference_btw_lastfeedback_posttest
  )

exploratory_moderators_plot_list= c(
  "participant_design", 
  "learning_type", 
  "posttest_item",
  "feedback_provided",  
  "feedback_remind_answer", 
  "nb_training_item", 
  "posttest_task", 
  "publication_year",
  "answer_time",
  "retention_interval"
)
    
moderator_plots_list=moderator_plots(for_moderator_plots_exploratory, "", moderators_list=exploratory_moderators_plot_list, threshold_for_moderator= 2, common_plot = FALSE, remove_small_groups=FALSE)

moderator_plots_list

```



### Function: Moderators Model

```{r}
calculate_moderator_models <- function(input_data, moderators, list_report = FALSE, file_path = getwd(), all_together = FALSE, threshold_for_moderator = 5, remove_small_groups = TRUE) {
  
  # Normalize and create the file path if it does not exist
  file_path <- normalizePath(file_path, mustWork = FALSE)
  
  if (!dir.exists(file_path)) {
    dir.create(file_path, recursive = TRUE)
    message("Directory created: ", file_path)
  }
  
  message("Saving .tex files to: ", file_path)
  message("Current working directory: ", getwd())
  
  moderator_models <- list()
  references <- list()  # Store reference info for each moderator

  # Loop through each moderator and create models
  for (moderator in moderators) {
    
    data <- input_data
    
    if (!moderator %in% colnames(data)) {
      warning("Moderator '", moderator, "' not found in the data. Skipping this moderator.")
      next
    }
    
    # Convert character variables to factors
    if (is.character(data[[moderator]])) {
      data[[moderator]] <- as.factor(data[[moderator]])
      message("Converted moderator '", moderator, "' from character to factor.")
    }
    
    # Remove small groups for categorical moderators
    if (remove_small_groups && is.factor(data[[moderator]])) {
      small_groups <- data %>%
        count(.data[[moderator]], name = "n") %>%
        filter(n < threshold_for_moderator) %>%
        pull(.data[[moderator]])
      
      if (!is.null(small_groups) && length(small_groups) > 0) {
        # Replace small groups with NA while preserving factor levels
        data[[moderator]][data[[moderator]] %in% small_groups] <- NA
        data[[moderator]] <- forcats::fct_drop(data[[moderator]])  # Drop unused levels
        message("Removed small groups from moderator '", moderator, "'.")
      }
    }
    
    # Drop NA values only for categorical moderators
    if (!is.numeric(data[[moderator]])) {
      data <- data %>%
        drop_na(.data[[moderator]])
    }
    
    # Calculate the reference category for this moderator after cleaning
    reference <- if (is.factor(data[[moderator]])) {
      levels(droplevels(data[[moderator]]))[1]  # Get first valid level after cleaning
    } else {
      NULL
    }
    
    # Store the reference for this moderator
    references[[moderator]] <- reference
    
    # Fit the model
    tryCatch({
      model <- robu(formula = as.formula(paste0("hedges_g ~ ", moderator)), data = data, 
                    studynum = unique_ID, var.eff.size = vi_g, 
                    rho = rho, modelweights = "HIER", small = FALSE)
      moderator_models[[moderator]] <- model
    }, error = function(e) {
      warning("An error occurred while fitting model for moderator '", moderator, "': ", e$message)
    })
  }
  
  if (all_together) {
    combined_moderators <- c()
    for (moderator in moderators) {
      if (!moderator %in% colnames(data)) {
        warning("Moderator '", moderator, "' not found in the data. Skipping this moderator for combined model.")
        next
      }
      
      tryCatch({
        combined_moderators <- c(combined_moderators, moderator)
        combined_formula <- paste(combined_moderators, collapse = " + ")
        combined_model <- robu(formula = as.formula(paste0("hedges_g ~ ", combined_formula)), data = data, 
                               studynum = unique_ID, var.eff.size = vi_g, 
                               rho = rho, modelweights = "HIER", small = FALSE)
        moderator_models[["moderator_all"]] <- combined_model
      }, error = function(e) {
        warning("An error occurred while adding moderator '", moderator, "' to the combined model: ", e$message)
        combined_moderators <- setdiff(combined_moderators, moderator)
      })
    }
  }
  
  if (!list_report) {
    return(moderator_models)
  }
  
  result_list <- list()
  
  for (moderator in names(moderator_models)) {
    model <- moderator_models[[moderator]]
    k <- nrow(model$data)  # Number of effect sizes
    reg_table <- model$reg_table
    
    reference <- references[[moderator]]
    
    # Format the labels: remove moderator name and append (vs reference)
    reg_table <- reg_table %>%
      mutate(
        labels = case_when(
          grepl("^X.Intercept.$", labels) ~ "(Intercept)",  # Format intercept
          !is.null(reference) ~ gsub(paste0("^", moderator), "", labels) %>%
            paste0(" (vs ", reference, ")"),  # Remove moderator name and append reference
          TRUE ~ labels
        )
      )
    
    subset_description <- paste0("Moderator Model- ", gsub("_", " ", moderator))
    
    results_table <- reg_table %>%
      mutate(
        Test = labels, 
        g = round(b.r, 2), 
        SE = round(SE, 2), 
        t = round(t, 2), 
        df = round(dfs, 2), 
        p = format.pval(prob, digits = 3, eps = 0.001, na.form = "<.001"), 
        `95 CI` = paste0("[", round(CI.L, 2), ", ", round(CI.U, 2), "]"), 
        sig = adjust_stars(sig) 
      ) %>%
      select(Test, g, SE, t, df, p, `95 CI`, sig)
    
    # Add n_clusters and n_obs as additional rows
    n_clusters <- model$N
    n_obs <- model$M
    summary_row <- tibble::tibble(
      Test = c("N clusters", "N observations"),
      g = NA, SE = NA, t = NA, df = NA, p = NA, `95 CI` = NA, sig = NA
    )
    summary_row$g <- c(n_clusters, n_obs)
    results_table <- bind_rows(results_table, summary_row)
    
    result_list[[moderator]] <- results_table
    
    # Create LaTeX table using kableExtra
    latex_table <- results_table %>%
      kbl(
        format = "latex",
        booktabs = TRUE,
        align = "lrrrrrrr"
      ) %>%
      kable_styling(
        latex_options = c("hold_position"), 
        full_width = FALSE
      ) %>%
      add_header_above(setNames(8, subset_description))  # Correct header format
    
    # Write the LaTeX table to a .tex file
    tex_filename <- normalizePath(file.path(file_path, paste0("moderator_analysis_", moderator, ".tex")), mustWork = FALSE)
    
    tryCatch({
      cat(latex_table, file = tex_filename)
      if (file.exists(tex_filename)) {
        message("File successfully created: ", tex_filename)
      } else {
        warning("Failed to create file: ", tex_filename)
      }
    }, error = function(e) {
      warning("An error occurred while writing file for moderator '", moderator, "': ", e$message)
    })
  }
  
  return(result_list)
}

```


Function: Big Table for Moderators Model

```{r}
generate_combined_latex_table <- function(moderator_model_output, output_name, file_path = getwd()) {
  
  # Ensure the file path is valid and normalize it
  file_path <- normalizePath(file_path, mustWork = FALSE)
  
  # Create directory if it does not exist
  if (!dir.exists(file_path)) {
    dir.create(file_path, recursive = TRUE)
    message("Directory created: ", file_path)
  }
  
  message("Saving LaTeX table to: ", file_path)
  
  # Combine and reshape data
  wide_table <- bind_rows(
    lapply(names(moderator_model_output), function(name) {
      df <- moderator_model_output[[name]]
      df %>%
        mutate(
          Variable = name,
          Row = row_number(),
          Combined = paste0(
            "", sprintf("%.3f", g), " [SE = ", sprintf("%.3f", SE), "]\n",
            "t = ", sprintf("%.3f", t), "\n",
            "p = ", p, " ", sig
          )
        ) %>%
        select(Row, Variable, Test, Combined)  # Keep only the necessary columns
    })
  ) %>%
  pivot_wider(
    names_from = Variable,
    values_from = Combined  # Use the combined column
  ) %>%
  select(-Row)  # Remove the Row column if not needed
  
  # Replace NA values with an empty string in the wide_table
  wide_table <- wide_table %>%
    mutate(across(everything(), ~ ifelse(is.na(.), "", .)))
  
  # Generate the rotated LaTeX table directly
  latex_table <- wide_table %>%
    kbl(
      format = "latex",
      booktabs = TRUE,  # Minimalistic borders
      linesep = "",     # Removes extra spacing between rows
      align = "l",      # Left-align all columns
      caption = "Moderator Effects on Feedback Timing"
    ) %>%
    kable_styling(
      latex_options = c("hold_position"),  # Keep position fixed
      full_width = FALSE,                 # Tables should not stretch full-width
      font_size = 10                      # Adjust font size to APA standards
    ) %>%
    footnote(
      general = "Results of meta-regressions for moderator variables.",
      threeparttable = TRUE
    ) %>%
    row_spec(0, bold = TRUE) %>%
    as.character()  # Convert to a LaTeX string
  
  # Write the LaTeX table to a .tex file
  tex_filename <- normalizePath(file.path(file_path, paste0("table_moderator_", output_name, ".tex")), mustWork = FALSE)
  
  tryCatch({
    cat(latex_table, file = tex_filename)
    
    if (file.exists(tex_filename)) {
      message("File successfully created: ", tex_filename)
    } else {
      warning("Failed to create file: ", tex_filename)
    }
  }, error = function(e) {
    warning("An error occurred while writing the LaTeX file for output '", output_name, "': ", e$message)
  })
}


```



## 7.2 Preregistered Moderators

```{r}
# Convert 'learning_domain' to a factor if it's not already
data_clean$feedback_type_simple_elaborated <- factor(data_clean$feedback_type_simple_elaborated)

# Relevel the factor to make "text_memory" the reference category
data_clean$feedback_type_simple_elaborated <- relevel(data_clean$feedback_type_simple_elaborated, ref = "simple")
```


### 7.2.a- Overall
no small number adjustments
```{r}
data_clean$educational_level <- factor(data_clean$educational_level)
data_clean$educational_level <- relevel(data_clean$educational_level, ref = "tertiary")
# Define the list of moderators
preregistered_moderators <- c(
  "feedback_type_simple_elaborated", 
  "delay_definition", 
  "feedback_delay_difference_converted_to_time_log",
  "educational_level",  
  "learning_domain", 
  "task_level", 
  "prior_knowledge", 
  "experimental_vs_curriculum_based_task"
)

# Run the function
moderator_models_preregistered <- calculate_moderator_models(input_data = data_clean, 
                                      moderators = preregistered_moderators, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = TRUE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = TRUE)

moderator_models_preregistered

generate_combined_latex_table(moderator_models_preregistered, "preregistered", file_path = getwd())
```

small number adjustment
```{r}
# Define the list of moderators
preregistered_moderators <- c(
  "feedback_type_simple_elaborated", 
  "delay_definition", 
  "feedback_delay_difference_converted_to_time_log",
  "educational_level_tertiary_others",  
  "learning_domain", 
  "task_level", 
  "prior_knowledge", 
  "experimental_vs_curriculum_based_task"
)

# Run the function
moderator_models_preregistered <- calculate_moderator_models(input_data = data_clean, 
                                      moderators = preregistered_moderators, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = TRUE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = TRUE)

moderator_models_preregistered

generate_combined_latex_table(moderator_models_preregistered, "preregistered", file_path = getwd())

```

### 7.2.b- Only in time based delays
```{r}
time_data = subset(data_clean, delay_definition == "seconds")

#Scale?
#time_data$feedback_delay_difference=scale(time_data$feedback_delay_difference)
#time_data$retention_interval_difference_btw_lastfeedback_posttest=scale(time_data$retention_interval_difference_btw_lastfeedback_posttest)

# Define the list of moderators
preregistered_moderators_for_time <- c(
  "feedback_type_simple_elaborated", 
  "feedback_delay_difference",
  "educational_level",  
  "learning_domain", 
  "task_level", 
  "prior_knowledge", 
  "experimental_vs_curriculum_based_task",
  "retention_interval_difference_btw_lastfeedback_posttest",
  "delayed_feedback_given_in",
  "feedback_delay_difference_converted_to_time_log"
)

# Run the function
moderator_models_preregistered_timeonly <- calculate_moderator_models(input_data = time_data, 
                                      moderators = preregistered_moderators_for_time, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = TRUE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = TRUE)

moderator_models_preregistered_timeonly

generate_combined_latex_table(moderator_models_preregistered_timeonly, "preregistered_timeonly", file_path = getwd())

```



### 7.2.c- Only in item based delays
```{r}

item_data =  subset(data_clean, delay_definition == "item")

#item_data$feedback_delay_difference=scale(item_data$feedback_delay_difference)
#item_data$feedback_delay_difference_converted=scale(item_data$feedback_delay_difference_converted)

# Define the list of moderators
preregistered_moderators_for_item <- c(
  "feedback_type_simple_elaborated", 
  "feedback_delay_difference",
  "educational_level",  
  "learning_domain", 
  "task_level", 
  "prior_knowledge", 
  "experimental_vs_curriculum_based_task",
  "feedback_delay_difference_converted",
  "feedback_delay_difference_converted_to_time_log"
)

# Run the function
moderator_models_preregistered_itemonly <- calculate_moderator_models(input_data = item_data, 
                                      moderators = preregistered_moderators_for_item, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = TRUE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = TRUE)

moderator_models_preregistered_itemonly

generate_combined_latex_table(moderator_models_preregistered_itemonly, "preregistered_itemonly", file_path = getwd())

```
combine them

```{r}
# Create a new list with only the data frames you want
combined_item_time <- list(
  feedback_delay_difference_converted_to_time_log = moderator_models_preregistered_timeonly$feedback_delay_difference_converted_to_time_log,
  feedback_delay_difference = moderator_models_preregistered_itemonly$feedback_delay_difference,
  feedback_delay_difference_converted = moderator_models_preregistered_itemonly$feedback_delay_difference_converted_to_time_log
)

combined_item_time


generate_combined_latex_table(combined_item_time, "combined_item_time", file_path = getwd())


```


###are they comparable?
```{r}
# Load required libraries
library(ggplot2)
library(broom)  # For tidying model results

# Assuming item_data is your data frame
# Example structure of item_data (replace this with your actual data frame)
# item_data <- read.csv("path/to/your/file.csv")

# Fit a linear model to calculate the statistics
model <- lm(feedback_delay_difference_converted ~ feedback_delay_difference, data = item_data)

# Extract the slope, intercept, p-value, and R-squared
model_stats <- broom::tidy(model)
r_squared <- summary(model)$r.squared
p_value <- model_stats$p.value[2]  # p-value for the slope
slope <- model_stats$estimate[2]   # slope of the regression line
intercept <- model_stats$estimate[1]  # intercept of the regression line

# Format the statistics to display them on the plot
stats_text <- paste0(
  "Slope = ", round(slope, 3), 
  "\nIntercept = ", round(intercept, 3), 
  "\nR² = ", round(r_squared, 3), 
  "\np = ", ifelse(p_value < 0.001, "< 0.001", round(p_value, 3))
)

# Create the scatter plot with regression line and annotated statistics
correlation_item_time=ggplot(item_data, aes(x = feedback_delay_difference, y = feedback_delay_difference_converted)) +
  geom_point(color = "#5E81AC", alpha = 0.6, size = 3) +  # Scatter points
  geom_smooth(method = "lm", color = "#893F45", se = TRUE) +  # Regression line with confidence interval
  labs(
    title = "",
    x = "Feedback Delay Difference (Item)",
    y = "Converted to Seconds"
  ) +
  annotate("text", 
           x = max(item_data$feedback_delay_difference, na.rm = TRUE) * 0.7, 
           y = max(item_data$feedback_delay_difference_converted, na.rm = TRUE) * 0.9, 
           label = stats_text, 
           hjust = 0, vjust = 1, size = 4, color = "black", fontface = "bold") +
plot_theme

correlation_item_time

ggsave("correlation_item_time.png", plot = correlation_item_time, width = 10, height = 8, dpi = 300)


```

## 7.3- Exploratory Moderators

```{r}
# Convert 'learning_domain' to a factor if it's not already
data_clean$time_limit_for_answer_include_not_reported <- factor(data_clean$time_limit_for_answer_include_not_reported)

# Relevel the factor to make "text_memory" the reference category
data_clean$time_limit_for_answer_include_not_reported <- relevel(data_clean$time_limit_for_answer_include_not_reported, ref = "answer_time_not_limited")
```

```{r}
# # Define the list of moderators
# exploratory_moderators <- c("participant_design", "arbitrary_learning","item_posttest", 
#                     "delayed_feedback_given_in", "retention_interval", "feedback_provided", "feedback_remind_answer", "nb_item_in_training", 
#                     "posttest_category", "learning_material_type_in_training","memory_retrieval_vs_knowledge_application", 
#                     "feedback_type_kr_kcr_ef_tryagain", "feedback_type_simple_elaborated","year","time_limit_for_answer", "time_limit_for_answer_include_not_reported",
#                     "feedback_i_d_rate","interval_btw_last_feedback_and_posttest_controlled", "prior_knowledge","feedback_delay_difference_converted_to_time_log", "retention_interval_difference_btw_lastfeedback_posttest"
# )

exploratory_moderators <- c("participant_design", "arbitrary_learning","item_posttest", 
                   "feedback_provided", "feedback_remind_answer", "nb_item_in_training", 
                    "memory_retrieval_vs_knowledge_application", "year", "time_limit_for_answer_include_not_reported",
                    "retention_interval_difference_btw_lastfeedback_posttest"
)

# Run the function
moderator_models_exploratory <- calculate_moderator_models(input_data = data_clean, 
                                      moderators = exploratory_moderators, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = FALSE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = TRUE)

moderator_models_exploratory


generate_combined_latex_table(moderator_models_exploratory, "exploratory", file_path = getwd())

```


## 7.4- Multiple MetaRegression (Only significant Moderators)

### 7.4.a- first regroup the minority categories

##### check learning_domain - whether its meaningful to merge "text_memory", "reading_comprehension"
```{r}
# Convert 'learning_domain' to a factor if it's not already
data_clean$learning_domain <- factor(data_clean$learning_domain)

# Relevel the factor to make "text_memory" the reference category
data_clean$learning_domain <- relevel(data_clean$learning_domain, ref = "text_memory")


moderator_learning_domain  <-  robu(formula = hedges_g ~ learning_domain, data = data_clean,
                     studynum = unique_ID, var.eff.size = vi_g,
                    rho = rho,  modelweights = "HIER", small = FALSE)

moderator_learning_domain

data_clean <- data_clean %>%
  mutate(learning_domain_formetareg = case_when(
    learning_domain %in% c("text_memory", "reading_comprehension") ~ "text_based_learning",
    TRUE ~ "others"  # Groups all other categories into "others"
  ))

# You can check the changes with
unique(data_clean$learning_domain_formetareg)

moderator_learning_domain  <-  robu(formula = hedges_g ~ learning_domain_formetareg, data = data_clean,
                     studynum = unique_ID, var.eff.size = vi_g,
                    rho = rho,  modelweights = "HIER", small = FALSE)

moderator_learning_domain

```

##### check educational level - whether its meaningful to merge "tertiary", "adult_education"
```{r}
# Convert 'learning_domain' to a factor if it's not already
data_clean$educational_level <- factor(data_clean$educational_level)

# Relevel the factor to make "text_memory" the reference category
data_clean$educational_level <- relevel(data_clean$educational_level, ref = "tertiary")


moderator_educational_level  <-  robu(formula = hedges_g ~ educational_level, data = data_clean,
                     studynum = unique_ID, var.eff.size = vi_g,
                    rho = rho,  modelweights = "HIER", small = FALSE)

moderator_educational_level

data_clean <- data_clean %>%
  mutate(educational_level_formetareg = case_when(
      educational_level %in% c("tertiary", "adult_education") ~ "tertiary+adult",
      educational_level %in% c("primary", "secondary") ~ "primary+secondary",
      TRUE ~ "other"  # This will categorize all remaining levels as "other"
  ))

# You can check the changes with
unique(data_clean$educational_level_formetareg)

# Convert 'learning_domain' to a factor if it's not already
data_clean$educational_level_formetareg <- factor(data_clean$educational_level_formetareg)

# Relevel the factor to make "text_memory" the reference category
data_clean$educational_level_formetareg <- relevel(data_clean$educational_level_formetareg, ref = "tertiary+adult")

moderator_educational_level  <-  robu(formula = hedges_g ~ educational_level_formetareg, data = data_clean,
                     studynum = unique_ID, var.eff.size = vi_g,
                    rho = rho,  modelweights = "HIER", small = FALSE)

moderator_educational_level

```

### 7.4.b- run the metareg

```{r}
# Define the list of moderators
#"educational_level_tertiary_others","learning_domain", "memory_retrieval_vs_knowledge_application", "year", "time_limit_for_answer"
significant_moderators <- c("educational_level_formetareg","learning_domain_formetareg", "memory_retrieval_vs_knowledge_application", "time_limit_for_answer_include_not_reported"
)

# Run the function
moderator_models_significant_moderators <- calculate_moderator_models(input_data = data_clean, 
                                      moderators = significant_moderators, 
                                      list_report = TRUE, 
                                      file_path = file_path,
                                      all_together = TRUE,
                                      threshold_for_moderator = threshold_for_moderator, remove_small_groups = FALSE)

moderator_models_significant_moderators

generate_combined_latex_table(moderator_models_significant_moderators, "significants", file_path = getwd())

```

```{r}
moderator_plots_list_significant=moderator_plots(data_clean, "", moderators_list=significant_moderators, threshold_for_moderator= 2, common_plot = FALSE, remove_small_groups=FALSE)

moderator_plots_list_significant
```

# 8- Correlation Plot

### Function: correlation or distribution (for binary variables)

```{r}
plot_correlations <- function(data, ref_variable, list_correlation_variables) {
  plots <- list()  # Initialize an empty list to store plots

  for (var in list_correlation_variables) {
    # Check if the variable is numeric or not
    if (is.numeric(data[[var]])) {
      # Fit the linear model for numeric variables
      model <- lm(as.formula(paste(ref_variable, var, sep=" ~ ")), data = data)
      # Extract model statistics
      model_stats <- broom::tidy(model)
      r_squared <- summary(model)$r.squared
      p_value <- model_stats$p.value[2]  # Assuming slope is the second coefficient
      slope <- model_stats$estimate[2]   # slope of the regression line
      intercept <- model_stats$estimate[1]  # intercept of the regression line

      # Format the statistics for display
      stats_text <- paste0(
        "Slope = ", round(slope, 3), 
        "\nIntercept = ", round(intercept, 3), 
        "\nR² = ", round(r_squared, 3), 
        "\np = ", ifelse(p_value < 0.001, "< 0.001", round(p_value, 3))
      )

      # Create the scatter plot with regression line
      plot <- ggplot(data, aes_string(x = ref_variable, y = var)) +
        geom_point(color = "#5E81AC", alpha = 0.6, size = 3) +
        geom_smooth(method = "lm", color = "#893F45", se = TRUE) +
        labs(x = ref_variable, y = var)
        # annotate("text", x = max(data[[ref_variable]], na.rm = TRUE) * 0.7, 
        #          y = max(data[[var]], na.rm = TRUE) * 0.9 ,
        #          label = stats_text, hjust = 0, vjust = 1, size = 4, color = "black", fontface = "bold"
        #          )
    } else {
      # Create the half-violin plot for non-numeric variables
      plot <- ggplot(data, aes_string(x = var, y = ref_variable, fill = var)) +
        geom_violin(trim = TRUE, scale = "area", draw_quantiles = NULL, adjust = 1, width = 0.8) +
        scale_y_continuous(limits = function(x) c(min(x, na.rm = TRUE) - diff(range(x, na.rm = TRUE)) * 0.1, max(x, na.rm = TRUE))) +
        geom_jitter(width = 0.05, color = "#4C566A", alpha = 0.9, size = 1.5) +  # Adding jittered points
        coord_flip() +
        labs(x = "", y = ref_variable)
      }
    
    # Add common labels and themes
    plot <- plot +
            theme_minimal() +
                        theme(axis.title = element_text(size = 12, face = "bold"),
                  plot.title = element_text(hjust = 0.5))+  # Hide legend
      plot_theme+
      theme(legend.position = "none")
    # Add plot to list
    plots[[var]] <- plot
  }
  
  return(plots)
}

```


### 8.1 year and other sig moderators

```{r}
ref_variable = "year"

#list_correlation_variables: 
#significant_moderators
#preregistered_moderators
#exploratory_moderators
# Call the function
correlation_plots <- plot_correlations(data_clean, ref_variable, list_correlation_variables=significant_moderators)

# Print the plots
if (!is.null(correlation_plots)) {
  for (plot_name in names(correlation_plots)) {
    print(correlation_plots[[plot_name]])
  }
}

```



